import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import warnings

warnings.filterwarnings('ignore')


class SFTFineTuner:
    """
    SFT kullanarak model ince ayarÄ± yapan sÄ±nÄ±f
    (Eski kÃ¼tÃ¼phanelerle uyumluluk iÃ§in standart 'transformers.Trainer' kullanÄ±lÄ±r)
    """

    def __init__(
            self,
            model_name: str = "google/gemma-2b",
            dataset_name: str = "timdettmers/openassistant-guanaco",
            output_dir: str = "./sft_model"
    ):
        self.model_name = model_name
        self.dataset_name = dataset_name
        self.output_dir = output_dir

        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        self.tokenizer = None
        self.model = None

        print(f" SFT Fine-Tuning BaÅŸlatÄ±lÄ±yor...")
        print(f" Tercih Edilen Cihaz: {self.device}")
        print(f" Model: {model_name}")
        print(f" Veri Seti: {dataset_name}")
        print(f"  Not: PyTorch modeli otomatik olarak en uygun cihaza taÅŸÄ±yabilir")

    def load_instruction_dataset(self, num_samples: int = 1000):
        """
        Veri setini yÃ¼kle (standart 'text' sÃ¼tununu kullanÄ±r)
        """
        print("\n Veri seti yÃ¼kleniyor...")
        dataset = load_dataset(self.dataset_name, split="train")
        dataset = dataset.select(range(min(num_samples, len(dataset))))

        dataset = dataset.train_test_split(test_size=0.1, seed=42)
        print(f" {len(dataset['train'])} eÄŸitim, {len(dataset['test'])} test Ã¶rneÄŸi yÃ¼klendi")
        if len(dataset['train']) > 0:
            print(f"\n Ã–rnek veri (birleÅŸtirilmiÅŸ 'text' formatÄ±):")
            print(dataset['train'][0]['text'][:300] + "...")
        return dataset

    def create_custom_dataset(self):
        """
        Ã–zel talimat veri seti oluÅŸtur (tek 'text' sÃ¼tunu olarak)
        """
        print("\n Ã–zel veri seti oluÅŸturuluyor...")

        if self.tokenizer is None:
            raise ValueError("HATA: create_custom_dataset Ã§aÄŸrÄ±lmadan Ã¶nce setup_model_and_tokenizer Ã§aÄŸrÄ±lmalÄ±dÄ±r.")

        instructions = [
            {"instruction": "Python'da liste comprehension nedir? AÃ§Ä±kla.",
             "response": "Liste comprehension, Python'da kÄ±sa ve okunabilir ÅŸekilde liste oluÅŸturmanÄ±n bir yoludur. Ã–rnek: squares = [x**2 for x in range(10)] ÅŸeklinde 0-9 arasÄ± sayÄ±larÄ±n karelerini iÃ§eren bir liste oluÅŸturabilirsiniz."},
            {"instruction": "Makine Ã¶ÄŸrenmesinde overfitting nedir?",
             "response": "Overfitting, bir modelin eÄŸitim verilerine aÅŸÄ±rÄ± uyum saÄŸlayÄ±p, yeni verilerde kÃ¶tÃ¼ performans gÃ¶stermesi durumudur. Regularization, dropout, ve daha fazla veri ile Ã¶nlenebilir."},
            {"instruction": "HTTP ve HTTPS arasÄ±ndaki fark nedir?",
             "response": "HTTPS, HTTP'nin gÃ¼venli versiyonudur. SSL/TLS protokolÃ¼ kullanarak veri ÅŸifrelemesi yapar. Web sitelerinde kilit simgesi gÃ¶rÃ¼yorsanÄ±z, HTTPS kullanÄ±lÄ±yor demektir."},
            {"instruction": "Bir binary search tree nedir?",
             "response": "Binary Search Tree (BST), her dÃ¼ÄŸÃ¼mÃ¼n en fazla 2 Ã§ocuÄŸu olan ve sol alt aÄŸacÄ±n deÄŸerlerinin kÃ¶kten kÃ¼Ã§Ã¼k, saÄŸ alt aÄŸacÄ±n deÄŸerlerinin ise bÃ¼yÃ¼k olduÄŸu bir veri yapÄ±sÄ±dÄ±r. Arama iÅŸlemleri O(log n) karmaÅŸÄ±klÄ±ÄŸÄ±ndadÄ±r."},
            {"instruction": "Docker nedir ve neden kullanÄ±lÄ±r?",
             "response": "Docker, uygulamalarÄ± container'lar iÃ§inde paketleyip Ã§alÄ±ÅŸtÄ±ran bir platformdur. AvantajlarÄ±: taÅŸÄ±nabilirlik, tutarlÄ± Ã§evre, kolay deployment, ve kaynak verimliliÄŸi. 'Works on my machine' sorununu Ã§Ã¶zer."},
            {"instruction": "Python'da decorator nedir?",
             "response": "Decorator, baÅŸka bir fonksiyonu saran ve davranÄ±ÅŸÄ±nÄ± deÄŸiÅŸtiren fonksiyondur. @decorator_name sÃ¶zdizimi ile kullanÄ±lÄ±r. Loglama, yetkilendirme ve Ã¶nbellekleme gibi durumlarda kullanÄ±ÅŸlÄ±dÄ±r."},
            {"instruction": "REST API tasarlarken nelere dikkat etmeliyim?",
             "response": "REST API tasarlarken ÅŸunlara dikkat edin: HTTP metodlarÄ±nÄ± doÄŸru kullanÄ±n (GET, POST, PUT, DELETE), anlamlÄ± URL yapÄ±larÄ± oluÅŸturun, uygun status kodlarÄ± dÃ¶ndÃ¼rÃ¼n, versiyonlama yapÄ±n ve gÃ¼venlik Ã¶nlemleri alÄ±n (HTTPS, authentication)."},
            {"instruction": "Git merge ve rebase arasÄ±ndaki fark nedir?",
             "response": "Git merge iki branch'i birleÅŸtirirken yeni bir commit oluÅŸturur ve geÃ§miÅŸi korur. Rebase ise commit'leri yeniden yazar ve daha temiz bir geÃ§miÅŸ oluÅŸturur. Merge gÃ¼venlidir, rebase geÃ§miÅŸi yeniden yazar."},
            {"instruction": "Makine Ã¶ÄŸrenmesinde regularization nedir?",
             "response": "Regularization, modelin overfitting yapmasÄ±nÄ± Ã¶nleyen bir tekniktir. L1 (Lasso) ve L2 (Ridge) regularization gibi yÃ¶ntemlerle model karmaÅŸÄ±klÄ±ÄŸÄ± cezalandÄ±rÄ±lÄ±r. Bu sayede model yeni verilerde daha iyi performans gÃ¶sterir."},
        ]

        formatted_data = []
        for item in instructions:
            text = f"### Human: {item['instruction']}\n### Assistant: {item['response']}"
            text += self.tokenizer.eos_token
            formatted_data.append({"text": text})

        dataset = Dataset.from_list(formatted_data)
        dataset = dataset.train_test_split(test_size=0.2, seed=42)
        print(f" {len(dataset['train'])} eÄŸitim, {len(dataset['test'])} test Ã¶rneÄŸi oluÅŸturuldu")
        return dataset

    def setup_model_and_tokenizer(self, use_quantization: bool = True):
        """Model ve tokenizer'Ä± hazÄ±rla"""
        print("\n Model ve tokenizer yÃ¼kleniyor...")

        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.tokenizer.padding_side = "right"

        if use_quantization and self.device == "cuda":
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )
        else:
            bnb_config = None

        print("   Model yÃ¼kleniyor...")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            dtype=torch.float16 if self.device == "cuda" else torch.float32,
            trust_remote_code=True,
        )

        actual_device = next(self.model.parameters()).device
        print(f"   Model yÃ¼klendi: {actual_device}")

        peft_config = LoraConfig(
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM",
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        )

        if use_quantization and self.device == "cuda":
            self.model = prepare_model_for_kbit_training(self.model)
        self.model = get_peft_model(self.model, peft_config)

        print(" Model hazÄ±r!")
        self.model.print_trainable_parameters()
        return self.model, self.tokenizer

    def create_sft_config(self, num_epochs: int = 3, batch_size: int = 4):
        print("\n  SFT konfigÃ¼rasyonu hazÄ±rlanÄ±yor...")

        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=4,
            learning_rate=2e-4,
            lr_scheduler_type="cosine",
            warmup_ratio=0.1,
            logging_steps=10,
            eval_strategy="steps",
            eval_steps=50,
            save_steps=100,
            save_total_limit=2,
            optim="paged_adamw_8bit" if self.device == "cuda" else "adamw_torch",
            fp16=True if self.device == "cuda" else False,
            gradient_checkpointing=False,
            report_to="none",
            logging_first_step=True,
            load_best_model_at_end=True,
        )
        print(f" KonfigÃ¼rasyon hazÄ±r!")
        return training_args

    def tokenize_dataset(self, dataset, max_length=512):
        """'text' sÃ¼tununu kullanarak veri setini tokenize eder"""
        print(f"  Tokenize ediliyor (max_length={max_length})...")

        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_length,
                padding=False,
                return_tensors=None,
            )

        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        return tokenized_dataset

    def train(self, dataset, training_args):
        """SFT eÄŸitimini baÅŸlat (Standart 'transformers.Trainer' ile)"""
        print("\n SFT EÄŸitimi BaÅŸlÄ±yor...")
        print("=" * 60)

        print(" 'transformers.Trainer' modu: 'trl' kullanÄ±lmÄ±yor.")

        max_seq_len = 512
        print(f"  â€¢ Max sequence length: {max_seq_len}")
        tokenized_train_dataset = self.tokenize_dataset(dataset["train"], max_length=max_seq_len)
        tokenized_eval_dataset = self.tokenize_dataset(dataset["test"], max_length=max_seq_len)

        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        print("   DataCollatorForLanguageModeling (mlm=False) kullanÄ±lÄ±yor.")

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_train_dataset,
            eval_dataset=tokenized_eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )

        print("\n EÄŸitim devam ediyor...")
        trainer.train()

        print("\n EÄŸitim tamamlandÄ±!")

        print(f"\n Model kaydediliyor: {self.output_dir}")
        trainer.save_model(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)

        return trainer

    def test_model(self, test_prompts: list):
        """EÄŸitilmiÅŸ modeli test et"""
        print("\n" + "=" * 60)
        print(" MODEL TESTÄ°")
        print("=" * 60)

        self.model.eval()

        model_device = next(self.model.parameters()).device
        print(f"ğŸ“ Model cihazÄ±: {model_device}")

        for prompt in test_prompts:
            print(f"\nğŸ“ Prompt: {prompt}")
            print("-" * 60)

            formatted_prompt = f"### Human: {prompt}\n### Assistant:"

            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(model_device)
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "### Assistant:" in response:
                response = response.split("### Assistant:")[-1].strip()

            print(f" Response:\n{response}")
            print("-" * 60)

    def compare_before_after(self, test_prompt: str):
        print("\n" + "=" * 60)
        print(" EÄITIM Ã–NCESÄ° vs SONRASI KARÅILAÅTIRMA")
        print("=" * 60)

        print("\n Orijinal model yÃ¼kleniyor...")
        original_model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map="auto",
            dtype=torch.float16 if self.device == "cuda" else torch.float32,
        )

        original_device = next(original_model.parameters()).device
        print(f"   Orijinal model yÃ¼klendi: {original_device}")

        model_device = next(self.model.parameters()).device
        original_device = next(original_model.parameters()).device

        formatted_prompt = f"### Human: {test_prompt}\n### Assistant:"

        print(f"\n Test Prompt: {test_prompt}\n")
        print(" EÄITIM Ã–NCESÄ° (Original Model):")
        print("-" * 60)

        inputs_original = self.tokenizer(formatted_prompt, return_tensors="pt").to(original_device)
        with torch.no_grad():
            outputs_before = original_model.generate(
                **inputs_original, max_new_tokens=150, temperature=0.7, do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        response_before = self.tokenizer.decode(outputs_before[0], skip_special_tokens=True)
        if "### Assistant:" in response_before:
            response_before = response_before.split("### Assistant:")[-1].strip()
        print(response_before)

        print("\n EÄITIM SONRASI (SFT Fine-tuned Model):")
        print("-" * 60)

        inputs_finetuned = self.tokenizer(formatted_prompt, return_tensors="pt").to(model_device)
        with torch.no_grad():
            outputs_after = self.model.generate(
                **inputs_finetuned, max_new_tokens=150, temperature=0.7, do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        response_after = self.tokenizer.decode(outputs_after[0], skip_special_tokens=True)
        if "### Assistant:" in response_after:
            response_after = response_after.split("### Assistant:")[-1].strip()
        print(response_after)

        print("\n" + "=" * 60)
        del original_model
        if self.device == "cuda":
            torch.cuda.empty_cache()


def main():
    print("=" * 60)
    print(" SFT (Supervised Fine-Tuning) [Plan C: Standart Trainer]")
    print("=" * 60)

    fine_tuner = SFTFineTuner(
        model_name="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        output_dir="./sft_tinyllama_model"
    )

    model, tokenizer = fine_tuner.setup_model_and_tokenizer(use_quantization=(fine_tuner.device == 'cuda'))

    print("\n Hangi veri setini kullanmak istersiniz?")
    print("  1. Guanaco (OpenAssistant) - Genel talimat takip")
    print("  2. Ã–zel veri seti - Teknik sorular")
    use_custom = True

    if use_custom:
        dataset = fine_tuner.create_custom_dataset()
    else:
        dataset = fine_tuner.load_instruction_dataset(num_samples=500)

    training_args = fine_tuner.create_sft_config(num_epochs=3, batch_size=4)

    trainer = fine_tuner.train(dataset, training_args)

    test_prompts = [
        "Python'da decorator nedir?",
        "REST API tasarlarken nelere dikkat etmeliyim?",
        "Git merge ve rebase arasÄ±ndaki fark nedir?"
    ]
    fine_tuner.test_model(test_prompts)

    fine_tuner.compare_before_after("Makine Ã¶ÄŸrenmesinde regularization nedir?")

    print("\n" + "=" * 60)
    print(" SFT Fine-Tuning TamamlandÄ±!")
    print("=" * 60)
    print(f"\n Model kaydedildi: {fine_tuner.output_dir}")


if __name__ == "__main__":
    main()